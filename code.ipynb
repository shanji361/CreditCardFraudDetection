{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the best hyperparameters...\n",
      "Best Parameters: {'classifier__learning_rate': 0.2, 'classifier__max_depth': None, 'classifier__n_estimators': 500}\n",
      "Training the best model...\n",
      "Evaluating the model...\n",
      "Mean F1 Score: 0.9800\n",
      "Standard Deviation: 0.0020\n",
      "Submission file 'submission.csv' has been created.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    R = 6371  # Radius of earth in kilometers\n",
    "   \n",
    "    # Convert decimal degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "   \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "   \n",
    "    return R * c\n",
    "\n",
    "\n",
    "class FraudDetectionModel:\n",
    "    def __init__(self, train_path, test_path):\n",
    "        \"\"\"\n",
    "        Initialize the fraud detection model with training and test data\n",
    "        \"\"\"\n",
    "        # Load the data\n",
    "        self.train_df = pd.read_csv(train_path)\n",
    "        self.test_df = pd.read_csv(test_path)\n",
    "       \n",
    "        # Preprocessing methods\n",
    "        self.fraud_counts = None\n",
    "        self.preprocess_data()\n",
    "       \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Comprehensive data preprocessing with additional features\n",
    "        \"\"\"\n",
    "        # Preprocess training data and get fraud counts\n",
    "        self.train_df, self.fraud_counts = self.enhanced_preprocess(self.train_df, is_train=True)\n",
    "       \n",
    "        # Preprocess test data using the fraud counts from training\n",
    "        self.test_df = self.enhanced_preprocess(self.test_df, is_train=False, fraud_counts=self.fraud_counts)\n",
    "       \n",
    "        # Prepare features and target for training\n",
    "        self.X = self.train_df.drop('is_fraud', axis=1)\n",
    "        self.y = self.train_df['is_fraud']\n",
    "       \n",
    "        # Prepare test data features\n",
    "        self.X_test = self.test_df\n",
    "   \n",
    "    def enhanced_preprocess(self, data, is_train=True, fraud_counts=None):\n",
    "        \"\"\"\n",
    "        Enhanced preprocessing function\n",
    "        \"\"\"\n",
    "        # Keep the 'id' column if it exists\n",
    "        ids = data['id'] if 'id' in data.columns else None\n",
    "       \n",
    "        if is_train:\n",
    "            # Group `cc_num` and calculate fraud counts\n",
    "            fraud_counts = data.groupby(['cc_num', 'is_fraud']).size().unstack(fill_value=0).reset_index()\n",
    "            fraud_counts.columns = ['cc_num', 'is_fraud_0_count', 'is_fraud_1_count']\n",
    "            # Add a new column for fraud_score\n",
    "            fraud_counts['fraud_score'] = (fraud_counts['is_fraud_0_count'] * 10) - (fraud_counts['is_fraud_1_count'] * 50)\n",
    "       \n",
    "        # Merge fraud counts into the data\n",
    "        data = data.merge(fraud_counts, on='cc_num', how='left')\n",
    "       \n",
    "        # Convert datetime columns\n",
    "        data['trans_datetime'] = pd.to_datetime(data['trans_date'] + ' ' + data['trans_time'])\n",
    "        data['dob'] = pd.to_datetime(data['dob'], errors='coerce')\n",
    "       \n",
    "        # Feature engineering\n",
    "        data['age'] = (data['trans_datetime'] - data['dob']).dt.days / 365.25\n",
    "        data['second'] = data['trans_datetime'].dt.second\n",
    "        data['minute'] = data['trans_datetime'].dt.minute\n",
    "        data['hour'] = data['trans_datetime'].dt.hour\n",
    "        data['day'] = data['trans_datetime'].dt.day\n",
    "        data['month'] = data['trans_datetime'].dt.month\n",
    "        data['weekday'] = data['trans_datetime'].dt.weekday\n",
    "       \n",
    "        # Calculate time-based feature\n",
    "        data['trans_time_seconds'] = data['trans_datetime'].dt.hour * 3600 + data['trans_datetime'].dt.minute * 60 + data['trans_datetime'].dt.second\n",
    "        data['seconds_from_midnight'] = 43200 - abs(43200 - data['trans_time_seconds'])\n",
    "       \n",
    "        # Calculate distance between cardholder and merchant\n",
    "        data['haversine_distance'] = haversine(\n",
    "            data['lat'], data['long'], data['merch_lat'], data['merch_long']\n",
    "        )\n",
    "       \n",
    "        # Select features\n",
    "        features = [\n",
    "            'amt', 'gender', 'category', 'job', 'state', 'city_pop',\n",
    "            'hour', 'day', 'month', 'weekday',\n",
    "            'age', 'haversine_distance', 'fraud_score'\n",
    "        ]\n",
    "       \n",
    "        if is_train:\n",
    "            features += ['is_fraud']\n",
    "       \n",
    "        data = data[features]\n",
    "       \n",
    "        # Convert categorical columns\n",
    "        categorical_cols = ['category', 'state', 'job']\n",
    "        gender_map = {'F': 0, 'M': 1}\n",
    "        data['gender'] = data['gender'].map(gender_map)\n",
    "       \n",
    "        # Label encoding for categorical columns\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            combined_data = pd.concat([data[col]], axis=0).astype(str)\n",
    "            le.fit(combined_data)\n",
    "            data[col] = le.transform(data[col].astype(str))\n",
    "       \n",
    "        # Impute missing values\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        data = pd.DataFrame(imputer.fit_transform(data), columns=features)\n",
    "       \n",
    "        # Add 'id' column back if it exists\n",
    "        if ids is not None:\n",
    "            data['id'] = ids\n",
    "       \n",
    "        if is_train:\n",
    "            return data, fraud_counts\n",
    "        else:\n",
    "            return data\n",
    "   \n",
    "    def create_preprocessing_pipeline(self):\n",
    "        \"\"\"\n",
    "        Create preprocessing pipeline\n",
    "        \"\"\"\n",
    "        # Identify column types\n",
    "        numeric_features = ['amt', 'hour', 'day', 'month', 'weekday', 'age', 'haversine_distance', 'fraud_score', 'city_pop']\n",
    "        categorical_features = ['gender', 'category', 'job', 'state']\n",
    "       \n",
    "        # Create preprocessors for numeric and categorical features\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "       \n",
    "        # Removed the problematic preprocessing steps\n",
    "        return None\n",
    "   \n",
    "    def find_best_hyperparameters(self, X, y):\n",
    "        param_grid = {\n",
    "            'classifier__n_estimators': [300, 500, 550, 600],\n",
    "            'classifier__max_depth': [None, 3, 10],\n",
    "            'classifier__learning_rate': [0.1, 0.2, 0.3],\n",
    "        }\n",
    "        pipeline = Pipeline([\n",
    "            ('classifier', XGBClassifier(eval_metric='logloss', random_state=42))\n",
    "        ])\n",
    "        grid_search = GridSearchCV(pipeline, param_grid, scoring='f1', cv=3)\n",
    "        grid_search.fit(X, y)\n",
    "        print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "        return grid_search.best_estimator_\n",
    "\n",
    "    def train_and_predict(self, random_state=42):\n",
    "        print(\"Finding the best hyperparameters...\")\n",
    "        best_model = self.find_best_hyperparameters(self.X, self.y)\n",
    "        print(\"Training the best model...\")\n",
    "        best_model.fit(self.X, self.y)\n",
    "        predictions = best_model.predict(self.X_test)\n",
    "        print(\"Evaluating the model...\")\n",
    "        self.evaluate_model(best_model, self.X, self.y)\n",
    "        return best_model, predictions\n",
    "\n",
    "    def evaluate_model(self, model, X, y, random_state=42):\n",
    "        cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "        f1_scores = cross_val_score(model, X, y, cv=cv, scoring='f1')\n",
    "        print(f\"Mean F1 Score: {f1_scores.mean():.4f}\")\n",
    "        print(f\"Standard Deviation: {f1_scores.std():.4f}\")\n",
    "\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    try:\n",
    "        fraud_model = FraudDetectionModel('train.csv', 'test.csv')\n",
    "        best_model, predictions = fraud_model.train_and_predict()\n",
    "        return fraud_model, best_model, predictions \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fraud_model, best_model, predictions = main()\n",
    "    if fraud_model and best_model and predictions is not None:\n",
    "        # Extract the 'id' column from the test dataset\n",
    "        test_ids = fraud_model.test_df['id']\n",
    "\n",
    "        # Create submission DataFrame\n",
    "        submission_df = pd.DataFrame({\n",
    "            'id': test_ids,\n",
    "            'is_fraud': predictions\n",
    "        })\n",
    "\n",
    "        # Save submission file\n",
    "        submission_df.to_csv('submission.csv', index=False)\n",
    "        print(\"Submission file 'submission.csv' has been created.\")\n",
    "    else:\n",
    "        print(\"Failed to generate submission file due to earlier errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'submission.csv' has been created.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
